{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cnn残差模型](img/cnn_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此分类模型是来自序列模型[Convolutional Sequence to Sequence Learning](https://arxiv.org/pdf/1705.03122.pdf)中的encoder部分，这里暂且叫它带残差的cnn model，如上图所示。\n",
    "\n",
    "    1.句子token和其对应的position经过embedding后，逐元素加和作为source embedding。\n",
    "    \n",
    "    2.source embedding进入： 线性层 -> 卷积块后得到的特征 -> 线性层。\n",
    "    \n",
    "    3.以上的输出和source embedding进行残差连接。\n",
    "    \n",
    "    4.以上的输出，我这里加了一个平均池化后进入线性层，预测输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cnn残差模型](img/cnn_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上是模型中的卷积块，可以设置多个卷积块。具体含义可以见论文[Convolutional Sequence to Sequence Learning](https://arxiv.org/pdf/1705.03122.pdf)。\n",
    "\n",
    "以上的图片和原始代码是改自https://github.com/bentrevett/pytorch-seq2seq 在此非常感谢作者实现了这么通俗易懂的代码架构，可以让其它人在上面进行修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch\\storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchtext import data,datasets\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "from torchtext.vocab import Vectors\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "intent_classification_path = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "# 训练数据路径\n",
    "train_data = os.path.join(intent_classification_path,'classification_data/knowledge_point_qa_data.csv')\n",
    "# 读取数据\n",
    "train_data = pd.read_csv(train_data)\n",
    "# 按字分    \n",
    "tokenize =lambda x: x.split(' ')\n",
    "\n",
    "TEXT = data.Field(\n",
    "                    sequential=True,\n",
    "                    tokenize=tokenize,\n",
    "                    lower=True,\n",
    "                    use_vocab=True,\n",
    "                    pad_token='<pad>',\n",
    "                    unk_token='<unk>',\n",
    "                    batch_first=True,\n",
    "                    fix_length=20)\n",
    "\n",
    "LABEL = data.Field(\n",
    "                    sequential=False,\n",
    "                    use_vocab=False)\n",
    "# 获取训练或测试数据集\n",
    "def get_dataset(csv_data, text_field, label_field, test=False):\n",
    "    fields = [('id', None), ('text', text_field), ('label', label_field)]\n",
    "    examples = []\n",
    "    if test: #测试集，不加载label\n",
    "        for text in csv_data['text']:\n",
    "            examples.append(data.Example.fromlist([None, text, None], fields))\n",
    "    else: # 训练集\n",
    "        for text, label in zip(csv_data['text'], csv_data['label']):\n",
    "            examples.append(data.Example.fromlist([None, text, label], fields))\n",
    "    return examples, fields\n",
    "\n",
    "train_examples,train_fields = get_dataset(train_data, TEXT, LABEL)\n",
    "\n",
    "train = data.Dataset(train_examples, train_fields)\n",
    "# 预训练数据\n",
    "pretrained_embedding = os.path.join(os.getcwd(), 'sgns.sogou.char')\n",
    "vectors = Vectors(name=pretrained_embedding)\n",
    "# 构建词典\n",
    "TEXT.build_vocab(train, min_freq=1, vectors = vectors)\n",
    "\n",
    "words_path = os.path.join(os.getcwd(), 'words.pkl')\n",
    "with open(words_path, 'wb') as f_words:\n",
    "    pickle.dump(TEXT.vocab, f_words)\n",
    "    \n",
    "BATCH_SIZE = 163\n",
    "# 构建迭代器\n",
    "train_iter = BucketIterator(\n",
    "                            dataset=train,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True,\n",
    "                            sort_within_batch=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([44, 300])\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "带残差的cnn\n",
    "'''\n",
    "class CNNResidual(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, intent_dim, hid_dim, n_layers, kernel_size, dropout, max_length=20):\n",
    "        super(CNNResidual, self).__init__()\n",
    "        \n",
    "        assert kernel_size % 2 == 1,'kernel size must be odd!' # 卷积核size为奇数，方便序列两边pad处理\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(DEVICE) # 确保整个网络的方差不会发生显著变化\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(input_dim, emb_dim) # token编码\n",
    "        self.pos_embedding = nn.Embedding(max_length, emb_dim) # token的位置编码\n",
    "        \n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim) # 线性层，从emb_dim转为hid_dim\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim) # 线性层，从hid_dim转为emb_dim\n",
    "        \n",
    "        # 卷积块\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels=hid_dim,\n",
    "                                              out_channels=2*hid_dim, # 卷积后输出的维度，这里2*hid_dim是为了后面的glu激活函数\n",
    "                                              kernel_size=kernel_size,\n",
    "                                              padding=(kernel_size - 1)//2) # 序列两边补0个数，保持维度不变\n",
    "                                              for _ in range(n_layers)]) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 利用encoder的输出进行意图识别\n",
    "        self.intent_output = nn.Linear(emb_dim, intent_dim)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src: [batch_size, src_len]\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        # 创建token位置信息\n",
    "        pos = torch.arange(src_len).unsqueeze(0).repeat(batch_size, 1).to(DEVICE) # [batch_size, src_len]\n",
    "        \n",
    "        # 对token与其位置进行编码\n",
    "        tok_embedded = self.tok_embedding(src) # [batch_size, src_len, emb_dim]\n",
    "        pos_embedded = self.pos_embedding(pos.long()) # [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        # 对token embedded和pos_embedded逐元素加和\n",
    "        embedded = self.dropout(tok_embedded + pos_embedded) # [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        # embedded经过一线性层，将emb_dim转为hid_dim，作为卷积块的输入\n",
    "        conv_input = self.emb2hid(embedded) # [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        # 转变维度，卷积在输入数据的最后一维进行\n",
    "        conv_input = conv_input.permute(0, 2, 1) # [batch_size, hid_dim, src_len]\n",
    "        \n",
    "        # 以下进行卷积块\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            # 进行卷积\n",
    "            conved = conv(self.dropout(conv_input)) # [batch_size, 2*hid_dim, src_len]\n",
    "            \n",
    "            # 进行激活glu\n",
    "            conved = F.glu(conved, dim=1) # [batch_size, hid_dim, src_len]\n",
    "            \n",
    "            # 进行残差连接\n",
    "            conved = (conved + conv_input) * self.scale # [batch_size, hid_dim, src_len]\n",
    "            \n",
    "            # 作为下一个卷积块的输入\n",
    "            conv_input = conved\n",
    "        \n",
    "        # 经过一线性层，将hid_dim转为emb_dim，作为enocder的卷积输出的特征\n",
    "        conved = self.hid2emb(conved.permute(0, 2, 1)) # [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        # 又是一个残差连接，逐元素加和输出，作为encoder的联合输出特征\n",
    "        combined = (conved + embedded) * self.scale # [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        # 意图识别,加一个平均池化,池化后的维度是：[batch_size, emb_dim]\n",
    "        intent_output = self.intent_output(F.avg_pool1d(combined.permute(0, 2, 1), combined.shape[1]).squeeze()) # [batch_size, intent_dim]\n",
    "        \n",
    "        return intent_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter [10/300], Loss: 2.1015\n",
      "iter [20/300], Loss: 1.9979\n",
      "iter [30/300], Loss: 1.9021\n",
      "iter [40/300], Loss: 1.7218\n",
      "iter [50/300], Loss: 3.7206\n",
      "iter [60/300], Loss: 1.5389\n",
      "iter [70/300], Loss: 1.4861\n",
      "iter [80/300], Loss: 0.9166\n",
      "iter [90/300], Loss: 2.8118\n",
      "iter [100/300], Loss: 1.1905\n",
      "iter [110/300], Loss: 0.5213\n",
      "iter [120/300], Loss: 0.6722\n",
      "iter [130/300], Loss: 0.2272\n",
      "iter [140/300], Loss: 0.0841\n",
      "iter [150/300], Loss: 0.0688\n",
      "iter [160/300], Loss: 0.0673\n",
      "iter [170/300], Loss: 0.0485\n",
      "iter [180/300], Loss: 0.0110\n",
      "iter [190/300], Loss: 0.0162\n",
      "iter [200/300], Loss: 0.0617\n",
      "iter [210/300], Loss: 0.0416\n",
      "iter [220/300], Loss: 0.0491\n",
      "iter [230/300], Loss: 0.0095\n",
      "iter [240/300], Loss: 0.0067\n",
      "iter [250/300], Loss: 0.0082\n",
      "iter [260/300], Loss: 0.0091\n",
      "iter [270/300], Loss: 0.0128\n",
      "iter [280/300], Loss: 0.0084\n",
      "iter [290/300], Loss: 0.0022\n",
      "iter [300/300], Loss: 0.0009\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(os.getcwd()+'/log', comment='cnnresidual')\n",
    "\n",
    "input_dim = len(TEXT.vocab)\n",
    "intent_dim = 9 # intent size\n",
    "emb_dim = TEXT.vocab.vectors.shape[1]\n",
    "hid_dim = 128\n",
    "conv_layers = 2 # 几层卷积块\n",
    "kernel_size = 3\n",
    "dropout = 0.25\n",
    "\n",
    "model = CNNResidual(input_dim, emb_dim, intent_dim, hid_dim, conv_layers, kernel_size, dropout).to(DEVICE)\n",
    "\n",
    "# 利用预训练模型初始化embedding，requires_grad=True，可以fine-tune\n",
    "model.tok_embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "\n",
    "# 训练模式\n",
    "model.train()\n",
    "\n",
    "# 优化和损失\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr=0.1, weight_decay=0.01)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.1, momentum=0.9, nesterov=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "with writer:\n",
    "    for iter in range(300):\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            train_text = batch.text\n",
    "            train_label = batch.label\n",
    "            train_text = train_text.to(DEVICE)\n",
    "            train_label = train_label.to(DEVICE)\n",
    "            out = model(train_text)\n",
    "            loss = criterion(out, train_label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (iter+1) % 10 == 0:\n",
    "                    print ('iter [{}/{}], Loss: {:.4f}'.format(iter+1, 300, loss.item()))\n",
    "            #writer.add_graph(model, input_to_model=train_text,verbose=False)\n",
    "            writer.add_scalar('loss',loss.item(),global_step=iter+1)\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "            \n",
    "model_path = os.path.join(os.getcwd(), \"model.h5\")\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cnn残差模型](img/loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
