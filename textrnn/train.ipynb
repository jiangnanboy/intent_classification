{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![textrnn模型](img/textrnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                       | 0/365076 [00:00<?, ?it/s]Skipping token b'365076' with 1-dimensional vector [b'300']; likely a header\n",
      "100%|████████████████████████████████████████████████████████████████████████| 365076/365076 [00:40<00:00, 8934.64it/s]\n",
      "D:\\Anaconda3\\lib\\site-packages\\torch\\storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchtext import data,datasets\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "from torchtext.vocab import Vectors\n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "intent_classification_path = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "# 训练数据路径\n",
    "train_data = os.path.join(intent_classification_path,'classification_data/classification_data.csv')\n",
    "# 读取数据\n",
    "train_data = pd.read_csv(train_data)\n",
    "# 按字分    \n",
    "tokenize =lambda x: x.split(' ')\n",
    "\n",
    "TEXT = data.Field(\n",
    "                    sequential=True,\n",
    "                    tokenize=tokenize,\n",
    "                    lower=True,\n",
    "                    use_vocab=True,\n",
    "                    pad_token='<pad>',\n",
    "                    unk_token='<unk>',\n",
    "                    batch_first=True,\n",
    "                    fix_length=20)\n",
    "\n",
    "LABEL = data.Field(\n",
    "                    sequential=False,\n",
    "                    use_vocab=False)\n",
    "# 获取训练或测试数据集\n",
    "def get_dataset(csv_data, text_field, label_field, test=False):\n",
    "    fields = [('id', None), ('text', text_field), ('label', label_field)]\n",
    "    examples = []\n",
    "    if test: #测试集，不加载label\n",
    "        for text in csv_data['text']:\n",
    "            examples.append(data.Example.fromlist([None, text, None], fields))\n",
    "    else: # 训练集\n",
    "        for text, label in zip(csv_data['text'], csv_data['label']):\n",
    "            examples.append(data.Example.fromlist([None, text, label], fields))\n",
    "    return examples, fields\n",
    "\n",
    "train_examples,train_fields = get_dataset(train_data, TEXT, LABEL)\n",
    "\n",
    "train = data.Dataset(train_examples, train_fields)\n",
    "# 预训练数据\n",
    "pretrained_embedding = os.path.join(os.getcwd(), 'sgns.sogou.char')\n",
    "vectors = Vectors(name=pretrained_embedding)\n",
    "# 构建词典\n",
    "TEXT.build_vocab(train, min_freq=1, vectors = vectors)\n",
    "\n",
    "words_path = os.path.join(os.getcwd(), 'words.pkl')\n",
    "with open(words_path, 'wb') as f_words:\n",
    "    pickle.dump(TEXT.vocab, f_words)\n",
    "    \n",
    "BATCH_SIZE = 163\n",
    "# 构建迭代器\n",
    "train_iter = BucketIterator(\n",
    "                            dataset=train,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True,\n",
    "                            sort_within_batch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 300])\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建分类模型\n",
    "class TextRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, output_size, dropout=0.5):\n",
    "        super(TextRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 这里batch_first=True，只影响输入和输出。hidden与cell还是batch在第2维\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size*2, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x :(batch, seq_len) = (163, 20)\n",
    "        # [batch,seq_len,embedding_dim] -> (163, 20, 300)\n",
    "        x = self.embedding(x) \n",
    "        #out=[batch_size, seq_len, hidden_size*2]\n",
    "        #h=[num_layers*2, batch_size, hidden_size]\n",
    "        #c=[num_layers*2, batch_size, hidden_size]\n",
    "        out,(h, c)= self.lstm(x)\n",
    "        # 最后时刻的hidden\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter [10/300], Loss: 2.7375\n",
      "iter [20/300], Loss: 2.7167\n",
      "iter [30/300], Loss: 2.7200\n",
      "iter [40/300], Loss: 2.7181\n",
      "iter [50/300], Loss: 2.7152\n",
      "iter [60/300], Loss: 2.7142\n",
      "iter [70/300], Loss: 2.7107\n",
      "iter [80/300], Loss: 2.7006\n",
      "iter [90/300], Loss: 2.6418\n",
      "iter [100/300], Loss: 2.3200\n",
      "iter [110/300], Loss: 2.1519\n",
      "iter [120/300], Loss: 2.0308\n",
      "iter [130/300], Loss: 2.3037\n",
      "iter [140/300], Loss: 2.1125\n",
      "iter [150/300], Loss: 1.9677\n",
      "iter [160/300], Loss: 1.8521\n",
      "iter [170/300], Loss: 1.8255\n",
      "iter [180/300], Loss: 1.7360\n",
      "iter [190/300], Loss: 1.7009\n",
      "iter [200/300], Loss: 1.5200\n",
      "iter [210/300], Loss: 2.2366\n",
      "iter [220/300], Loss: 1.3887\n",
      "iter [230/300], Loss: 1.2162\n",
      "iter [240/300], Loss: 1.0565\n",
      "iter [250/300], Loss: 0.8792\n",
      "iter [260/300], Loss: 0.7540\n",
      "iter [270/300], Loss: 0.6081\n",
      "iter [280/300], Loss: 0.5910\n",
      "iter [290/300], Loss: 0.4691\n",
      "iter [300/300], Loss: 0.3708\n",
      "iter [310/300], Loss: 0.3033\n",
      "iter [320/300], Loss: 0.2526\n",
      "iter [330/300], Loss: 0.2226\n",
      "iter [340/300], Loss: 0.2168\n",
      "iter [350/300], Loss: 0.1647\n",
      "iter [360/300], Loss: 0.1403\n",
      "iter [370/300], Loss: 0.1242\n",
      "iter [380/300], Loss: 0.0933\n",
      "iter [390/300], Loss: 0.0965\n",
      "iter [400/300], Loss: 0.0592\n",
      "iter [410/300], Loss: 0.1430\n",
      "iter [420/300], Loss: 0.0605\n",
      "iter [430/300], Loss: 0.0411\n",
      "iter [440/300], Loss: 0.0747\n",
      "iter [450/300], Loss: 0.0293\n",
      "iter [460/300], Loss: 0.0190\n",
      "iter [470/300], Loss: 0.0196\n",
      "iter [480/300], Loss: 0.0179\n",
      "iter [490/300], Loss: 0.0113\n",
      "iter [500/300], Loss: 0.0102\n",
      "iter [510/300], Loss: 0.0094\n",
      "iter [520/300], Loss: 0.0087\n",
      "iter [530/300], Loss: 0.0168\n",
      "iter [540/300], Loss: 0.0049\n",
      "iter [550/300], Loss: 0.0046\n",
      "iter [560/300], Loss: 0.0051\n",
      "iter [570/300], Loss: 0.0028\n",
      "iter [580/300], Loss: 0.0025\n",
      "iter [590/300], Loss: 0.0021\n",
      "iter [600/300], Loss: 0.0020\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(os.getcwd()+'/log', comment='textrnn')\n",
    "\n",
    "# 训练\n",
    "\n",
    "# 构建model\n",
    "model = TextRNN(len(TEXT.vocab), TEXT.vocab.vectors.shape[1], 128, 2, 16).to(DEVICE)\n",
    "# 利用预训练模型初始化embedding，requires_grad=True，可以fine-tune\n",
    "model.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "# 训练模式\n",
    "model.train()\n",
    "# 优化和损失\n",
    "# optimizer = torch.optim.Adam(model.parameters(),lr=0.1, weight_decay=0.1)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.1, momentum=0.95, nesterov=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "with writer:\n",
    "    for iter in range(600):\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            train_text = batch.text\n",
    "            train_label = batch.label\n",
    "            train_text = train_text.to(DEVICE)\n",
    "            train_label = train_label.to(DEVICE)\n",
    "            out = model(train_text)\n",
    "            loss = criterion(out, train_label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (iter+1) % 10 == 0:\n",
    "                    print ('iter [{}/{}], Loss: {:.4f}'.format(iter+1, 300, loss.item()))\n",
    "            #writer.add_graph(model, input_to_model=train_text,verbose=False)\n",
    "            writer.add_scalar('loss',loss.item(),global_step=iter+1)\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "            \n",
    "model_path = os.path.join(os.getcwd(), \"model.h5\")\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![loss](img/loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
